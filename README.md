# kawk

## Идея

Есть очень старый, но довольно востребованный язык AWK.

На нем пишутся скрипты типа `awk 'BEGIN {FS = ","} $1=="hello"  {print NR, $2 + $3, $2 * $3 }'}`

Вот конкретно этот скрипт, получив на стандартный вход строки

```
hello,12,23
bye,234,123
vasya,1223,432
hello,33,22
```

породит на выходе

```
1 35 276
4 55 726
```

потому что 

- мы ему указали в блоке BEGIN, что разделителем полей будет запятая (вообще у него есть понятие переменных и переменная FS выделена как специальная, хранящая знание о разделителе полей)
- он стал читать стандартный вход и разбивать его на записи (по умолчанию по переводу строки)
- каждую запись он стал разбивать на поля (в данном случае по запятой)
- он стал отбирать записи по критерию "первое поле равно 'hello'"
- для каждого такого поля стал печатать номер записи (хранящийся в переменной NR и автоматически обновляющийся), а также сумму и произведение второго и третьего полей

Язык используется и по сей день.
Лично я использую его если не ежедневно, то уж еженедельно точно.

У него есть хорошая ниша - для тех одномоментных задач, когда на питончике писать не очень хочется, а sed-а не хватает.

Но у него есть недостаток - он очень минималистичен в плане ожиданий от входных данных.
Например, он не понимает CSV во всей его сложности. Когда разделитель может стать частью поля или когда истользуется многострочное текстовое поле.

Поэтому хочется его развить путем переписывания с нуля на JVM-языке. А там открываются перспективы - плагин-архитектура для поддержки разных входных форматов
(разных логик разбиения на записи и поля), доступ к возможностям JVM-библиотек (например, чтобы получив из входных данных URL с помощью конструкции вроде 
`url.download`

Все это мы делать за одну домашку не будем, но пути наметим и каркас приложения построим.

## Ограничения и пожелания

Про мутабельность жесткого запрета нет. Но использование ее там, где легко обойтись без нее - это минус.

Если вы хорошо владеете темой парсинга - можете парсить как-то по-своему. Но это должен быть код на Котлине и
ваш код. Не сгенерированный ANTLR-ом или чем-то еще. Не вызванный из сторонней библиотеки.

Если хотите парсер-комбинаторпами - можно ими. Но своими, не сторонне-библиотечными.

Придерживайтесь общих принципов: single responsibility, separation of concerns, DontRepeatYourself, ограничение видимости.

Постарайтесь без оверинжениринга. Я указываю на места, где желательны и уместны абстрагирование и универсализация. В других местах стоит подумать, так ли нужны многослойные абстракции.

По тестированию требования не так жестки, как в первой, но я буду проверять на тестах длинне, чем 10 символов. Падения будут минусом, а приложенные юнит-тесты - плюсом (не факт, что непосредственно в виде баллов, но точно улучшат впечатление).


## Задание 1

20 баллов

Реализуем лексический анализатор.

Лексический анализатор преобразует последовательность символов в последовательность лексем.

Часто он же занимается отбрасыванием комментариев.

Какие тут комментарии могут быть ? Иногда awk-скрипты пишут в файлах. И там они бывают уместны.

Будем в стиле скриптовых языков использовать '#' в качестве однострочного комментария. Заодно можно будет при написании
скриптов использовать https://en.wikipedia.org/wiki/Shebang_(Unix).

И многострочные тоже хотим - через `/* */`. Без контроля за вложенностью.

Но если разрешить `/* */`, то это может восприниматься как "C++/Java-style". И напрашивается вариант `//`. Разрешим и так.

При обработке комментариев важно не потерять данные о строках в исходном тексте - для адекватной диагностики.

Какие нам нужны лексемы:
- операции: `+`, `-`, `*`, `/`, `%`, `==`, `!=`, `>`, `<`
- присваивание: `=`
- спецсимволы: запятая, фигурные скобки, точка с запятой, круглые скобки
- идентификаторы: `[$_a-zA-Z][_a-zA-Z0-9]*`, кроме ключевых слов
- ключевые слова: `BEGIN`, `END` (case sensitive)
- строковые литералы: `"nsjshjs"`, `\"` - кавычка, `\\` - одиночный `\`, или же  `r"vjjgh"` - без интерпретации `\`
- целые литералы: `65`, `0x1245F`, `0b11101`, `100_000`, восьмеричные не надо,  чтобы влезали в `Long`
- fixed-point литералы: `123.45`, `123.`, `.45`, никакой scientific notation (в awk есть, но как-то не хочется в ее тонкости влезать), максимум 20 символов до десятичной точки, не считая ведущих нулей, максимум `10` после, не считая хвостовых нулей

Хотя скрипты могут приходить из файлов, вряд ли это будут миллионы строк кода. Поэтому мы можем считать, что на момент создания анализатора у нас есть весь исходник для анализа в виде строки.

В базовом сценарии нам как будто не нужно много анализаторов сразу. Но при развитии языка вполне можно придумать какую-нибудь модульность и тогда несколько анализаторов один за другим могут пригодиться.

Поэтому пусть будет такой интерфейс, не закладывающийся на синглтонность:

```
interface Lexer : Sequence<Token>

data class Pos(val line: Int, val col: Int)

enum class TokenType { 
    OPERATION, ASSIGN, SPECIAL,
    IDENTIFIER, KEYWORD, STRING, INT, FIXED_POINT
}

interface Token {
    val type: TokenType
    val repr: String // представление в исходном тексте
    val pos: Pos
}
 
class TextLexer(src: String): Lexer {
    override fun iterator(): Iterator<Token> {
        TODO("Not yet implemented")
    }
}
```

На уровне лексического анализа могут обнаруживаться ошибки о них надо сообщать.

Два основных требования:

- сообщения должны быть базового информативными и разные ошибки должны давать отличимые сообщения
- сообщение должно указывать местоположение проблемы в терминах "номер строки и номер колонки". Номер строки/колонки начинается с 1.

## Задание 2

30 баллов

Поверх лексического анализатора реализуем синтаксический.

### Подход

Эта задача посложнее, потому что тут недостаточно по текущей позиции понять, что тут за конструкция находится, и немножко заглянуть вперед. Тут начинаются рекурсивно вложенные конструкции.

Язык может быть формально описан грамматикой, то есть набором правил, определяющим, является ли входная последовательность корректной с точки зрения данного языка.

Пример грамматики можно видеть здесь: https://en.wikipedia.org/wiki/Recursive_descent_parser#Example_parser

По грамматике можно построить синтаксический анализатор. Для развитых языков программирования они строятся автоматически парсер-генераторами. Но у нас язык простой и мы напишем его (парсер) сами методом рекурсивного спуска.

В примере грамматики по ссылке если красные и зеленые элементы. Красные - это терминальные символы, базовые элементы. Зеленые - нетерминальные. По сути они задают некую структуру языка. А чисто формально - один из нетерминальных символов является стартовым.
И синтаксический анализ - это процесс построения цепочки подстановок, в котором на каждом шаге нетерминальный символ заменяется на цепочку из правой части одного из правил грамматики (в котором он стоит в левой части).

### Грамматика: начало

Термнальные символы - это лексемы, полученные из лексического анализатора. Иногда к ним удобно добавить маркер конца - `EOF`.

В нашем случае стартовый символ можно определить так:

```
program ::= cond_block* EOF

cond_block ::= cond? block

cond ::= BEGIN | END | expr

block ::= { sentences }
```

(Эта нотация называется расширенной формой Бекуса-Наура. Она кажется интуитивно понятной. Если что-то неясно, можно почитать здесь или спросить меня).

### Рекурсивный спуск

Мы пишем парсер по этим правилам.

Отпарсить программу - отпарсить последовательность условных блоков (возможно, пустую) и увидеть за ней конец потока лексем.

Пришел BEGIN или END - идем распознавать блок.

Пришло что-то, похожее на условное выражение - пытаемся отпарсить его. И так далее. Некоторые конструкции могут вкладываться сами в себя, отсюда и рекурсивность. А спуск - потому что идет от самого высокоабстрактного нетерминала к лексемам. (Бывают методы, которые наоборот - идут от лексем, ищут правила и пытаются построить дерево разбора "снизу вверх").

Подробнее про метод можно почитать здесь: https://en.wikipedia.org/wiki/Recursive_descent_parser.

Мы же доопределим язык.

### Грамматика: продолжение

Синтаксически у нас может быть несколько блоков одного типа. И может не быть блоков вообще.

```
sentences ::= | sentence | ';' sentences
```

Предложений может не быть, может быть одно или несколько. 

Точки с запятой - в стиле Паскаля. Как в C не хочется. Перевод строки считать за точку с запятой - слишком сложная история для реализации.

```
sentence ::= func_call | assignment

func_call ::= IDENTIFIER  '(' params ')' | IDENTIFIER  param
params := | param | ';' params
param ::= expr

assignment ::= IDENTIFIER '=' expr
```

На этапе синтаксического анализа мы не знаем и не можем знать, есть ли у нас функция с данным именем. А вот если кто-то пытается
вызвать функцию `BEGIN` - это уже синтаксическая ошибка.

### Грамматика: выражения

Важно правильно определить выражение.

Наивно это можно сделать так:

```
expr ::= INT_LITERAL |      // 1
         expr '+' expr |    // 2
         expr '-' expr |    // 3
         expr '*' expr |    // 4
         expr '/' expr |    // 5
         (expr)             // 6
```
Наивность здест в том, что выражение `3 * 2 + 1` может быть выведено двумя способами: можно считать `3` выражением, которое умножается на выражение `2 + 1`, а можно считать за выражение `3 * 2`, которое складывается с выражением `1`. А от структуры вывода зависит структура построенного дерева.
А от структуры дерева - порядок вычислений.

Решение можно увидеть здесь:
https://en.wikipedia.org/wiki/Recursive_descent_parser#Example_parser

По сути - мы заводим отдельные правила для операций разных приоритетов. Оно выглядит не очень масштабируемым - нам же нужно рекурсивную функцию писать на каждое правило. И все довольно однотипные. Но на наш язык - пойдет.


### Постановка задачи

Здесь я не прописываю конкретные структуры.

Они на ваше усмотрение и их разумность тоже будет оцениваться.

В рамках этого задания надо сделать синтаксический анализатор, который принимает строку-программу и возвращает объект - ее 
дерево разбора.

Анализатор может быть функцией или вызываемым объектом.

Возвращаемое дерево должно уметь себя напечатать. Не через `toString` по умолчанию, а в каком-то YAML-подобном виде. И это - тоже часть задания, довольно несложная (каждый узел рекурсивно печатает себя со смещением, пропорционально его глубине).
Чтобы глазами виделась структура.

Семантика и интерпретация не входят в задания. Но собрать нужную информацию в правильной структуре - входит.

Поэтому здесь важно разобраться с приоритетами и убедить в этом через печать дерева.

Дереву здесь достаточно просто повторять структуру разбора.

Если вам удобно сразу при разборе как-то его преобразовывать в абстрактную форму - можно сразу и печатать преобразованное.

Любой синтаксически некорректный вход должен диагностировться как ошибка. Семантически точные сообщения об ошибках при парсинге - очень непростая задача и идеала не требуется. Но не надо "Error" на все случаи печатать. Восстанавиваться от ошибок разбора не обязательно (это тоже непросто) - достаточно плймать первую и сломаться. 

### Бонусное задание

20-30 баллов

Пререквизит - хорошо сделанное (с претензией на 25+баллов) основное.

Обобщите логику работы с приоритетами в рекурсивном спуске. Чтобы не было по правилу и по рекурсивной функции на приоритет.
Чтобы можно было привязывать приоритет к лексеме-операции, смотреть на него и обходиться одной рекурсией.

Если с этим разобраться - можно легко расширить набор побитовыми операциями, сдвигами и т.п.

А еще круче - разобраться с ассоциативностью. У нас тут все лево-ассоциативное. И ради этого я присваивание сделал отдельным оператором.

А если с ней еще обобщенно разобраться - можно и присваивания как операции, и `+=` добавить.


## Задание 3

20 баллов

Дерево синтаксического разбора еще не очень удобно. Например дерево выражения `1 + 2 + 3 + 4 + 5` будет бинарным и немного перекошенным.

Реализуем функцию (ну или вызываемый объект), который преобразует дерево синтаксического разбора в дерево абстрактного синтаксиса.

Ожидаемый минимум:

- цепочка операций с одним приоритетом превращается в один узел с нужным числом узлов
- вызов функции - отдельный узел с детьми-параметрами
- аналогично со `statements`
- BEGIN/END/cond - блоки группируются по интересам. У корня потенциально три ребенка. У каждого произвольное число детей - в порядке их описания
- никаких скобок (они и при парсинге легко отбрасываются, но уж в AST им точно нечего делать)

Что еще захотите - тоже можно.

И чтоб два раза не вставать: проведем свертку констант.
Чем умнее - тем лучше. Как минимум - '1 + 2' можно сразу посчитать. А если у нас AST - то и '1 + v + 2' - тоже можно (Нв самом деле тут все сложно, потому что от порядка зависит возможность переполнения, но будем считать что для непереполняющихся незавимо от порядка все хорошо, а если переполнение зависит от порядка - мы не будем ничего гарантировать).

Сементика операций:

- `int` и `int` дают `int`, деление - целочисленное
- `fixed-point` - это `double` во внутреннем представлении
- `double` и число дают `double`
- строка + строка - не конкатенация.

Мы же хотим как awk. А в awk конкатенация - это просто строка за строкой (и мы это не поддерживаем в домашке - чтобы грамматику не усложнять). Каждую строку пытаемся отпарсить как `int` или `fixed-point`. С одним унарным минусом и одним унарным плюсом, если они есть. Если уперлись во что-то, не попадающее в ожидания, - останавливаемся и что напарсили, то и будет значением. Если ничего - `0`.

Примеры:

`'123' + 12` будет `134`

`'-123' + '12'` будет `-112`

`'--123' + '12q'` будет `12`

`'123.qq' + '12'` будет `134.0`


Ну и c другими операциями так же

Здесь сдаваемым артефактом является функция, которая порождает дерево. Она может вызываться как часть процесса компиляции.
Но должна быть возможность по тексту получить дерево и его напечатать в читаемом виде.

## Задание 4

20 баллов

Научимся как-то исполнять наш код.

Мы не хотим просто повторить awk. Мы хотим его приспособить к разным источникам данных.

Заведем абстракцию "источник данных". Она(абстракция)/он(источник) будет поставлять поток записей.
Запись - это текст. И он делится на поля. По каким-то настраиваемым правилам.
Потенциально источник может быть бесконечным.

И заведем сущность, которая по источнику данных и по AST сможет исполнить скрипт.

Скрипт, из которого порождается AST, может приходить из командной строки непосредственно или из файла (если на месте скрипта указано что-то вроде `@f.awk`).

Блоки `BEGIN` исполняются перед чтением источника в порядке их определения.

Блоки `END` исполняются после завершения чтения источника (если оно случится) в порядке их определения.

После чтения каждой записи делается попытка по очереди исполнить все условные блоки.

Если условие не указано - это `true`.

Если указано - вычисляется. Если получилась строка, то пустая - это `false`, непустая - `true`. Если не строка, то это число.
`0 - false`, не `0 - true`. В частности, `"0" - true`, а `"0" + 0 - false`. И `"hello" + 0` - тоже.

У нас есть одна предопределенная функция `print`. Она печатает аргументы, перемажая пробелами. Вызов другой функции - ошибка времени исполнения.

Если читаем несуществующую переменную - она читается как пустая строка.

Есть предопределенные пременные:

- `NR`: номер записи, считается с `1`
- `$0` - содержимое текущей записи
- `$1` - содержимое первого поля
- `$2, $3, ...` - по количеству полей
- `NF` - количество полей в текущей записи

Обычная переменная, получившая значение, держит его вплоть до блока `END`.

Присваивание значения предопределенной переменной в awk может иметь интересную сементику, но мы ее воплотим в отдельном задании.

А здесь мы воплотим наш частный случай абстракций для ситауции чтения из текстового файла и разделения записей по переводу строки, а полей - по пробельным последовательностям.

## Задание 5

10 баллов

Реализуем семантику присваивания встроенным переменным.

Как минимум - они "зеркалируют" то, что им присвоили.


`NF = 2` - оставим в записи два поля. Если снова увеличим - они не "вернутся"

```
$ awk ' { print $0; NF = 2 ; print $0 ; NF = 3 ; print $0  }'
11 22 33 44 55 - вход

11 22 33 44 55
11 22
11 22
```

`$9 = 12` - расширяет запись, если такого поля не было
```
$ awk ' { print $0; $9="hello" ; print $0 ;  print NF   }'
11 22 33 - вход

11 22 33 
11 22 33      hello
9
```

Если что-то присваивается в `$0` - пересчитываются все поля и `NF`.

То есть они все - `view` над неким объектом и присваивание меняет состояние этого объекта.

Изменение `NR` не меняет порядок обработки, но `NR` следующей записи будет инкрементом того, что было на момент окончания предыдущей.

Вот это все и надо реализовать.

